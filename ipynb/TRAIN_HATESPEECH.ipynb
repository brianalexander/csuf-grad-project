{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRAIN_HATESPEECH",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpA1bOds0aIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cee972-d004-4357-fbd1-51cc9efe202e"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install torch\n",
        "!pip install torchmetrics\n",
        "!pip install ipywidgets\n",
        "!pip install IProgress"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/brian/.local/lib/python3.8/site-packages (4.6.0)\n",
            "Requirement already satisfied: packaging in /home/brian/.local/lib/python3.8/site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/brian/.local/lib/python3.8/site-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: sacremoses in /home/brian/.local/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/brian/.local/lib/python3.8/site-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/brian/.local/lib/python3.8/site-packages (from transformers) (4.60.0)\n",
            "Requirement already satisfied: filelock in /home/brian/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/brian/.local/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /home/brian/.local/lib/python3.8/site-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/brian/.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /home/brian/.local/lib/python3.8/site-packages (from sacremoses->transformers) (0.14.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pytorch-lightning in /home/brian/.local/lib/python3.8/site-packages (1.3.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.4.0 in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (2021.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/lib/python3/dist-packages (from pytorch-lightning) (5.3.1)\n",
            "Requirement already satisfied: torchmetrics>=0.2.0 in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (4.60.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/lib/python3/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: torch>=1.4 in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (1.8.0+cu111)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (2.4.1)\n",
            "Requirement already satisfied: packaging in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (20.9)\n",
            "Requirement already satisfied: pyDeprecate==0.3.0 in /home/brian/.local/lib/python3.8/site-packages (from pytorch-lightning) (0.3.0)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (2.22.0)\n",
            "Requirement already satisfied: aiohttp in /home/brian/.local/lib/python3.8/site-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (3.7.4.post0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.37.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/lib/python3/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.34.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (56.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/brian/.local/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.30.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/brian/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/brian/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/brian/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/brian/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /home/brian/.local/lib/python3.8/site-packages (from torch>=1.4->pytorch-lightning) (3.10.0.0)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/lib/python3/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/brian/.local/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (5.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/brian/.local/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/brian/.local/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (20.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/brian/.local/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (1.6.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning) (2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/brian/.local/lib/python3.8/site-packages (from packaging->pytorch-lightning) (2.4.7)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in /home/brian/.local/lib/python3.8/site-packages (1.2.4)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /home/brian/.local/lib/python3.8/site-packages (from pandas) (1.18.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /home/brian/.local/lib/python3.8/site-packages (1.18.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /home/brian/.local/lib/python3.8/site-packages (1.8.0+cu111)\n",
            "Requirement already satisfied: numpy in /home/brian/.local/lib/python3.8/site-packages (from torch) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions in /home/brian/.local/lib/python3.8/site-packages (from torch) (3.10.0.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torchmetrics in /home/brian/.local/lib/python3.8/site-packages (0.3.1)\n",
            "Requirement already satisfied: packaging in /home/brian/.local/lib/python3.8/site-packages (from torchmetrics) (20.9)\n",
            "Requirement already satisfied: numpy in /home/brian/.local/lib/python3.8/site-packages (from torchmetrics) (1.18.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /home/brian/.local/lib/python3.8/site-packages (from torchmetrics) (1.8.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /home/brian/.local/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (3.10.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/brian/.local/lib/python3.8/site-packages (from packaging->torchmetrics) (2.4.7)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: ipywidgets in /home/brian/.local/lib/python3.8/site-packages (7.6.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /home/brian/.local/lib/python3.8/site-packages (from ipywidgets) (5.5.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/brian/.local/lib/python3.8/site-packages (from ipywidgets) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/brian/.local/lib/python3.8/site-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/brian/.local/lib/python3.8/site-packages (from ipywidgets) (5.0.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /home/brian/.local/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /home/brian/.local/lib/python3.8/site-packages (from ipywidgets) (7.23.1)\n",
            "Requirement already satisfied: tornado>=4.2 in /home/brian/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
            "Requirement already satisfied: jupyter-client in /home/brian/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: decorator in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.0.7)\n",
            "Requirement already satisfied: matplotlib-inline in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: pygments in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.9.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython>=4.0.0->ipywidgets) (4.6.0)\n",
            "Requirement already satisfied: backcall in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.18)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (56.1.0)\n",
            "Requirement already satisfied: pickleshare in /home/brian/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/brian/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/brian/.local/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
            "Requirement already satisfied: ipython-genutils in /home/brian/.local/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /home/brian/.local/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /home/brian/.local/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.3.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.14.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /home/brian/.local/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
            "Requirement already satisfied: wcwidth in /home/brian/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /home/brian/.local/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.3.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /home/brian/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.0.3)\n",
            "Requirement already satisfied: argon2-cffi in /home/brian/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
            "Requirement already satisfied: jinja2 in /home/brian/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.2)\n",
            "Requirement already satisfied: nbconvert in /home/brian/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
            "Requirement already satisfied: Send2Trash>=1.5.0 in /home/brian/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
            "Requirement already satisfied: prometheus-client in /home/brian/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.10.1)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /home/brian/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.7.3)\n",
            "Requirement already satisfied: ptyprocess in /home/brian/.local/lib/python3.8/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /home/brian/.local/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /home/brian/.local/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/lib/python3/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /home/brian/.local/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /home/brian/.local/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: bleach in /home/brian/.local/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.3.0)\n",
            "Requirement already satisfied: defusedxml in /home/brian/.local/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /home/brian/.local/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/brian/.local/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.3)\n",
            "Requirement already satisfied: testpath in /home/brian/.local/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/lib/python3/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
            "Requirement already satisfied: nest-asyncio in /home/brian/.local/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: async-generator in /home/brian/.local/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
            "Requirement already satisfied: packaging in /home/brian/.local/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.9)\n",
            "Requirement already satisfied: webencodings in /usr/lib/python3/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/brian/.local/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: IProgress in /home/brian/.local/lib/python3.8/site-packages (0.4)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from IProgress) (1.14.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45z2fqdffQZ9"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJFTchyfXcZA"
      },
      "source": [
        "# Python\n",
        "import json\n",
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "# General 3rd Party\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# HuggingFace\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast, BertConfig\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.functional import F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# PyTorch Lightning\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer, loggers, seed_everything\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "import torchmetrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeKxnIjqgqNw",
        "outputId": "16a7a1a1-cd9a-4f37-ba38-7c8fe6b628a1"
      },
      "source": [
        "seed_everything(42, workers=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLMoHcIvGR3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d701a845-ea2b-4e83-e503-b483978ce60c"
      },
      "source": [
        "MODEL_DIRECTORY = '/home/brian/Documents/kubernetes/models/hatespeech'\n",
        "\n",
        "directories_needed = [MODEL_DIRECTORY]\n",
        "\n",
        "twitter = \"/home/brian/Documents/kubernetes/datasets/live/hatespeech/labeled_data.csv\"\n",
        "gab = \"/home/brian/Documents/kubernetes/datasets/live/hatespeech/gab.csv\"\n",
        "reddit = \"/home/brian/Documents/kubernetes/datasets/live/hatespeech/reddit.csv\"\n",
        "\n",
        "\n",
        "files_needed = [twitter, gab, reddit]\n",
        "\n",
        "def assertFilesAndDirectoriesExist(files, directories):\n",
        "  for d in directories:\n",
        "    assert os.path.isdir(d)\n",
        "\n",
        "  for f in files:\n",
        "    assert os.path.exists(f)\n",
        "  \n",
        "  print(\"+ All files and directories accounted for!\")\n",
        "\n",
        "assertFilesAndDirectoriesExist(files_needed, directories_needed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ All files and directories accounted for!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWzxe_WLQiAc"
      },
      "source": [
        "## Load Data from Disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIpBl1LLJVMD"
      },
      "source": [
        "twitter_frame = pd.read_csv(twitter)\n",
        "gab_frame = pd.read_csv(gab, usecols=[\"text\", \"hate_speech_idx\"])\n",
        "reddit_frame = pd.read_csv(reddit, usecols=[\"text\", \"hate_speech_idx\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHY1pcxMD2el"
      },
      "source": [
        "def expandDataset(row,row_accumulator,separator):\n",
        "  # print(row['hate_speech_idx'], row['hate_speech_idx'] == float(\"nan\"), type(row['hate_speech_idx']))\n",
        "  hate_speech_inds = []\n",
        "  if (isinstance(row['hate_speech_idx'], str)):\n",
        "    hate_speech_inds = json.loads(row['hate_speech_idx'])\n",
        "  # print(type(row['text']))\n",
        "  text_arr = row['text'].split(\"\\n\")\n",
        "  for ind in range(len(text_arr)):\n",
        "    new_row = {}\n",
        "    if (text_arr[ind] == \"\"):\n",
        "        continue\n",
        "    if (ind+1 in hate_speech_inds):\n",
        "      new_row['label'] = 1\n",
        "      new_row['text'] = str.strip(text_arr[ind])\n",
        "      row_accumulator.append(new_row)\n",
        "    else:\n",
        "      new_row['label'] = 0\n",
        "      new_row['text'] = text_arr[ind]\n",
        "      row_accumulator.append(new_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-ACl6kETk_"
      },
      "source": [
        "new_rows = []\n",
        "reddit_frame.apply(expandDataset, axis=1,args=(new_rows, \"\\n\"))\n",
        "reddit_frame_parsed = pd.DataFrame(new_rows)[['text', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "57Z_dsUJI8Em",
        "outputId": "6a9c4006-8954-46ef-c6b1-8c9cd38bb6bc"
      },
      "source": [
        "reddit_frame_parsed.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  1. A subsection of retarded Hungarians? Ohh bo...      1\n",
              "1  2. \\tHiii. Just got off work. 444 is mainly th...      0\n",
              "2  3. \\t\\twow i guess soyboys are the same in eve...      0\n",
              "3  4. \\t\\t\\tOwen Benjamin's soyboy song goes for ...      0\n",
              "4  1. > \"y'all hear sumn?\"  by all means I live i...      0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1. A subsection of retarded Hungarians? Ohh bo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2. \\tHiii. Just got off work. 444 is mainly th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3. \\t\\twow i guess soyboys are the same in eve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4. \\t\\t\\tOwen Benjamin's soyboy song goes for ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1. &gt; \"y'all hear sumn?\"  by all means I live i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHj_pI3pEJH4"
      },
      "source": [
        "new_rows = []\n",
        "gab_frame.apply(expandDataset, axis=1,args=(new_rows, \"\\n\"))\n",
        "gab_frame_parsed = pd.DataFrame(new_rows)[['text', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q5FGGF7mI6YC",
        "outputId": "a88581dd-e18b-4b94-e84e-136052d389e3"
      },
      "source": [
        "gab_frame_parsed.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  1. i joined gab to remind myself how retarded ...      1\n",
              "1  1. This is what the left is really scared of. ...      0\n",
              "2  2. \\tThat literally looks like a monkey. Why a...      0\n",
              "3                                   3. \\t\\tDumb Cunt      1\n",
              "4                        1. It makes you an asshole.      0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1. i joined gab to remind myself how retarded ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1. This is what the left is really scared of. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2. \\tThat literally looks like a monkey. Why a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3. \\t\\tDumb Cunt</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1. It makes you an asshole.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgPdO3x_loR8"
      },
      "source": [
        "twitter_frame['label'] = twitter_frame['hate_speech'].apply(lambda x: 1 if x > 0 else 0)\n",
        "twitter_frame_parsed = twitter_frame[['tweet', 'label']].rename(columns={'tweet': 'text'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bubann-ymAzm",
        "outputId": "bbf14b3e-c9b5-457b-ac4a-5e7949577c48"
      },
      "source": [
        "twitter_frame_parsed.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  !!! RT @mayasolovely: As a woman you shouldn't...      0\n",
              "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      0\n",
              "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...      0\n",
              "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      0\n",
              "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...      0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7lZH8cjKCL3"
      },
      "source": [
        "full_frame = pd.concat([reddit_frame_parsed, gab_frame_parsed, twitter_frame_parsed], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIdJhLpDj9BW"
      },
      "source": [
        "full_frame = full_frame.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "op87CUpKKkk7",
        "outputId": "9d6d17c3-9534-4179-d176-be02ba324037"
      },
      "source": [
        "full_frame.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label\n",
              "5763            1. @Patriotic1  Cunt cunt cunt cunt LOL!      1\n",
              "19124  RT @iAmDaHarper: he's saying...he doesn't disc...      0\n",
              "2741              @CallMeDaishaa ghetto ass Pocahontas .      1\n",
              "27238  2. \\tdon't let that commie nigger into the Guv...      1\n",
              "3210               @Fugazi3011 \"leave you beaner retard\"      1"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5763</th>\n",
              "      <td>1. @Patriotic1  Cunt cunt cunt cunt LOL!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19124</th>\n",
              "      <td>RT @iAmDaHarper: he's saying...he doesn't disc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2741</th>\n",
              "      <td>@CallMeDaishaa ghetto ass Pocahontas .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27238</th>\n",
              "      <td>2. \\tdon't let that commie nigger into the Guv...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3210</th>\n",
              "      <td>@Fugazi3011 \"leave you beaner retard\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I01-AxwMAF1"
      },
      "source": [
        "inputs = full_frame['text'].values\n",
        "labels = full_frame['label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8otF-OnEQ19k"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg92IyPopeV8"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU6_eSi_EmbM"
      },
      "source": [
        "class TextLabelTokenizerDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"Offensive Language Dataset\"\"\"\n",
        "\n",
        "  def __init__(self, inputs, labels, tokenizer):\n",
        "    super().__init__()\n",
        "\n",
        "    self.inputs = inputs\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.inputs)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    encoding = self.tokenizer(\n",
        "        self.inputs[idx],\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=500\n",
        "    )\n",
        "\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "    attention_mask = encoding[\"attention_mask\"]\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": input_ids.type(torch.long),\n",
        "        \"attention_mask\": attention_mask.type(torch.long),\n",
        "        \"target\": torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMxan7eRNHhQ"
      },
      "source": [
        "text_label_tokenizer_dataset = TextLabelTokenizerDataset(inputs, labels, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TMAhzlvB3xC"
      },
      "source": [
        "class CustomDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, dataset, batch_size: int = 32):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dataset = dataset\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def prepare_data(self):\n",
        "    # download\n",
        "    pass\n",
        "\n",
        "  def setup(self, stage: Optional[str] = None):\n",
        "    print(\"DATA MODULE SETUP\")\n",
        "    train_size = int(0.6 * len(self.dataset))\n",
        "    val_size = int(0.2 * len(self.dataset))\n",
        "    test_size = len(self.dataset) - train_size - val_size\n",
        "\n",
        "    self.train, self.val, self.test = random_split(\n",
        "        self.dataset, \n",
        "        [train_size, val_size, test_size]\n",
        "        )\n",
        "\n",
        "  def train_dataloader(self):\n",
        "      return DataLoader(self.train, batch_size=self.batch_size, num_workers=16, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "      return DataLoader(self.val, batch_size=self.batch_size, num_workers=16)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "      return DataLoader(self.test, batch_size=self.batch_size, num_workers=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUu-kJGalZlJ"
      },
      "source": [
        "custom_datamodule = CustomDataModule(text_label_tokenizer_dataset, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0YT3WszSXxV"
      },
      "source": [
        "# Model Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqU1iOmjehcw"
      },
      "source": [
        "class BertForSequenceClassificationLM(pl.LightningModule):\n",
        "  def __init__(self, pretrained='bert-base-uncased', bert_weights=None, freeze=True, lr=3e-5, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lr = lr\n",
        "    self.eps = eps\n",
        "\n",
        "    config = BertConfig.from_pretrained(pretrained)\n",
        "    config.num_labels = 1\n",
        "\n",
        "    self.classifier = BertForSequenceClassification(\n",
        "        config\n",
        "        )\n",
        "    \n",
        "    if (bert_weights):\n",
        "      weights = torch.load(bert_weights)\n",
        "      self.classifier.bert = weights\n",
        "\n",
        "    # self.classifier = BertForSequenceClassification.from_pretrained(\n",
        "    #     pretrained\n",
        "    #     )\n",
        "    if (freeze):    \n",
        "      for param in self.classifier.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    self.train_loss = []\n",
        "    self.val_loss = []\n",
        "    self.val_acc = []\n",
        "    self.test_loss = []\n",
        "    self.test_acc = []\n",
        "\n",
        "    self.loss = F.binary_cross_entropy_with_logits\n",
        "\n",
        "    self.validation_accuracy = torchmetrics.Accuracy()\n",
        "    self.test_accuracy = torchmetrics.Accuracy()\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    return self.classifier(\n",
        "        input_ids=input_ids, \n",
        "        attention_mask=attention_mask, \n",
        "        return_dict=True\n",
        "        )['logits']\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    b_input_ids = batch['input_ids'].view(batch['input_ids'].shape[0], -1)\n",
        "    b_attention_masks = batch[\"attention_mask\"].view(batch['attention_mask'].shape[0], -1)\n",
        "    b_targets = batch[\"target\"].unsqueeze(1)\n",
        "\n",
        "    logits = self(b_input_ids, b_attention_masks)\n",
        "\n",
        "    loss = self.loss(logits, b_targets)\n",
        "\n",
        "    self.log('train_loss', loss, on_step=True, on_epoch=True, \n",
        "             prog_bar=True)\n",
        "\n",
        "    return {'loss': loss}\n",
        "  \n",
        "  def training_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "\n",
        "    self.log(f'epoch_train_loss_{self.current_epoch}', avg_loss)\n",
        "    self.train_loss.append(avg_loss)\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    b_input_ids = batch['input_ids'].view(batch['input_ids'].shape[0], -1)\n",
        "    b_attention_masks = batch[\"attention_mask\"].view(batch['attention_mask'].shape[0], -1)\n",
        "    b_targets = batch[\"target\"].unsqueeze(1)\n",
        "\n",
        "    logits = self(b_input_ids, b_attention_masks)\n",
        "\n",
        "    preds = torch.sigmoid(logits)\n",
        "\n",
        "    val_acc = torch.mean(((preds > 0.5) == b_targets).to(torch.float))\n",
        "    val_loss = self.loss(logits, b_targets)\n",
        "\n",
        "    logs = {'val_loss': val_loss, 'val_acc': val_acc}\n",
        "\n",
        "    self.log('val_loss', val_loss, on_step=True, on_epoch=True, \n",
        "             prog_bar=True)\n",
        "    \n",
        "    return logs\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "    avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "\n",
        "    self.log(f'epoch_val_accuracy', avg_acc, on_epoch=True, prog_bar=True)\n",
        "    self.log(f'epoch_val_loss', avg_loss, on_epoch=True, prog_bar=True)\n",
        "    print(\"val_acc\", avg_acc)\n",
        "    print(\"val_loss\", avg_loss)\n",
        "    \n",
        "    self.val_acc.append(avg_acc)\n",
        "    self.val_loss.append(avg_loss)\n",
        "  \n",
        "  def test_step(self, batch, batch_idx):\n",
        "    b_input_ids = batch['input_ids'].view(batch['input_ids'].shape[0], -1)\n",
        "    b_attention_masks = batch[\"attention_mask\"].view(batch['attention_mask'].shape[0], -1)\n",
        "    b_targets = batch[\"target\"].unsqueeze(1)\n",
        "\n",
        "    logits = self(b_input_ids, b_attention_masks)\n",
        "\n",
        "    preds = torch.sigmoid(logits)\n",
        "\n",
        "    test_acc = torch.mean(((preds > 0.5) == b_targets).to(torch.float))\n",
        "    test_loss = self.loss(logits, b_targets)\n",
        "\n",
        "    logs = {'test_loss': test_loss, 'test_acc': test_acc}\n",
        " \n",
        "    self.log('test_loss', test_loss, on_step=True, on_epoch=True, \n",
        "             prog_bar=True)\n",
        "    \n",
        "    return logs\n",
        "  \n",
        "  def test_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "    avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
        "\n",
        "    self.log(f'epoch_test_accuracy', avg_acc, on_epoch=True, prog_bar=True)\n",
        "    self.log(f'epoch_test_loss', avg_loss, on_epoch=True, prog_bar=True)\n",
        "    \n",
        "    self.test_acc.append(avg_acc)\n",
        "    self.test_loss.append(avg_loss)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = AdamW(self.parameters(), lr=self.lr, eps=self.eps)\n",
        "\n",
        "    return [optimizer]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsBnwb8RUHoY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq9LDFHnVqNg"
      },
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='epoch_val_accuracy',\n",
        "    dirpath=MODEL_DIRECTORY,\n",
        "    filename='hatespeech-{epoch:02d}-{epoch_val_accuracy:.6f}',\n",
        "    save_top_k=1,\n",
        "    mode='max',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crvQcbpgkv3b",
        "outputId": "729d6ab2-2a94-4a64-d282-8a79840929f0"
      },
      "source": [
        "EPOCHS = 4\n",
        "\n",
        "trainer = Trainer(gpus=1, max_epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkIHQKtekv06"
      },
      "source": [
        "model = BertForSequenceClassificationLM(bert_weights=\"/home/brian/Documents/kubernetes/models/base.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMCqq68_IyZi",
        "outputId": "e786d8b5-7fc8-4fb1-d2fe-cbcef2b22a63"
      },
      "source": [
        "model.train(mode=True)\n",
        "print(\"setting training mode...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setting training mode...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfQ7orDSy4-l"
      },
      "source": [
        "trainer.fit(model, custom_datamodule)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}